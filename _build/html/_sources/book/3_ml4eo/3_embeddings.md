(structure:index)=
# 3 - Embeddings

This chapter is written entirely by Leif Denby and was created for SENSE CDT training at University of Leeds on 6th March 2024 and made openly available at [github](https://github.com/leifdenby/SENSE_convml_tt). The neural network model developed in [L Denby (2020)](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019GL085190) is used to explore the concept of embedding spaces. 

The exercises are broken down as follows:

1. Dimensionality reduction: Examine how the neural network has used the embedding space; are all 100 dimensions necessary? Can we identify what features the neural network has learnt by comparing tiles in different parts of the embedding space? notebook: 1a_Use_PCA_analysis_to_study_tile_embeddings.ipynb

2. High-dimensional clustering: use different clustering methods to study the extent to which the neural network has formed distinct clusters in the embedding space. notebook: 1b_Exploring_embedding_space_with_clustering_methods.ipynb

3. Using your own input data: either by generating synthetic input tiles or using your own data source you will work with the pre-trained model to study whether the trained neural network groups them together in the embedding space. notebook: 2_Working_with_your_own_data.ipynb


```{tableofcontents}
```